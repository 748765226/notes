https://blog.csdn.net/ly18846826264/article/details/106948921/?utm_medium=distribute.pc_relevant.none-task-blog-title-1&spm=1001.2101.3001.4242

如果根据神经网络模型的计算单位对其进行分类，则可以区分三个不同的时代。

第一代 以McCulloch-Pitts neurons麦卡洛克-皮茨神经元作为计算单元，单层感知机，多层感知机；第二代在神经元的基础上增加了一个激活函数sigmoid，如果不用激励函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合 ， 　如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中；第三代spiking neurons

人工神经网络中的神经元的特征在于单个、静态、连续值的激活。然而生物神经元使用离散的脉冲来计算和传输信息，并且除了脉冲发放率外，脉冲时间也很重要。因此脉冲神经网络（SNN）在生物学上比ANN更现实，并且如果有人想了解大脑的计算方式，它无疑是唯一可行的选择。



尽管DNN在历史上是受大脑启发的，但与大脑相比，它们的结构、神经计算和学习规则存在根本差异。最重要的**区别之一是信息在其单元之间传播的方式**，正是这一观察催生了脉冲神经网络（SNN）领域的诞生。在大脑中，神经元之间的通信是通过**传播动作电位序列（也称为传到下游神经元的脉冲序列）**来完成的。这些单独的脉冲在时间上很稀疏，因此每个脉冲都有很高的信息含量，并且近似具有均匀的幅值（100 mV，脉冲宽度约为1ms）。因此SNN中的信息是通过脉冲时序传达的，包括延迟和脉冲速率，也可能通过群。与实际的生理机制相比，SNN几乎普遍使用理想化的脉冲生成机制。



作为非脉冲DNN的ANN使用连续值激活来进行通信。尽管DNN的能源效率可能会提高，但是SNN在这方面提供了特殊的机会，因为如上所述，脉冲事件在时间上具有稀疏性。**脉冲网络还具有对生物神经系统中发生的信息传递的时间特性固有敏感的优势**。研究表明，每个脉冲的精确时序对于大脑的多个区域都是高度可靠的，并暗示了在神经编码中的重要作用。脉冲活动中的这种精确的时间模式被认为是感觉信息处理区域和大脑中神经运动控制区域的关键编码策略。



在科学动机方面，公认的是，**大脑在嘈杂的环境中识别复杂的视觉模式或识别听觉目标的能力是深层脉冲网络中嵌入的多个处理阶段和多种学习机制的结果。**



**深度神经模型学习中的重要部分，包括脉冲和非脉冲，都发生在特征发现的层次结构中，在这个层次结构中，获取的特征越来越复杂且具有区别性、抽象性和不变性。**



原则上，受生物启发的SNN具有比传统的速率编码网络更高的表示能力和容量。此外，**SNN允许使用一种生物启发式的学习（权重修改），该学习方式取决于直接连接的神经元对之间的脉冲的相对时序，在这些连接中，权重修改所需的信息可局部的。**这种局部学习类似于在大脑许多地方发生的显著性的学习。



**向脉冲网络配备多层学习是一个开放的领域，可以极大地提高其在不同任务上的性能。先前研究的主要核心是基于这样一个事实，即以脉冲的定时进行编码会携带有用的信息，并在生物系统中展现出强大的计算能力。**



在过去的几十年中，作为强大的第三代神经网络，SNN的引入激励了许多研究，重点是基于生物学的模式识别方法。 **SNN最初是受到大脑和神经元进行信息转换所使用的通信方案的启发，该转换通过自适应突触中时间上离散的动作电位（脉冲）完成。在生物神经元中，由突触前刺激引起的膜电位变化的总和超过阈值时，会产生脉冲。脉冲产生的速率和脉冲序列的时间模式携带有关外部刺激的信息和正在进行的计算。 SNN使用非常相似的过程来生成脉冲和信息。**



####  SNN结构 

SNN体系结构由脉冲神经元和互连的突触组成，这些突触由可调节的标量权重建模。**实现SNN的第一步是使用基于速率的方法、某种形式的时间编码或群体编码将模拟输入数据编码为脉冲序列。**如前所述，大脑中的生物神经元（和仿真的脉冲神经元类似）从神经网络中的其他神经元接收突触输入。

####  SNN学习规则

在几乎所有的神经网络中，无论是脉冲还是非脉冲，**学习都是通过调整标量值的突触权重实现的。**脉冲使不能在非脉冲网络中直接复制的生物可解释的学习规则变得可行。**神经科学家已经确定了该学习规则的许多变体，这些变体总体上讲属于脉冲时序依赖可塑性（STDP）。它的关键特征是，根据突触前和突触后神经元的相对脉冲时间，在大约数十毫秒的时间间隔内调整连接突触前神经元的权重。**用于执行权重调整的信息对于突触来说既是局部的，在时间上也是局部的

#### 突触可塑性

在神经科学中，突触可塑性（Synaptic plasticity）是指神经细胞间的连接，即突触，其连接强度可调节的特性。

在人工神经网络中，突触可塑性是指利用神经科学中突触可塑性有关理论结合数学模型来构造神经元之间的联系。

大脑中的学习可以理解为突触连接强度随时间的变化过程，这种能力称为突触可塑性（synaptic plasticity）

##### 无监督学习

无监督学习根据局部事件进行处理，局部事件没有需要解决的任务的任何概念，同时也不没有变好或变坏的概念，学习过程仅仅依靠局部活动进行调整。

1949年，Hebb（赫布）**【突触前神经元向突触后神经元的持续重复的刺激可以导致突触传递效能的增加。】**对突触连接调整的假设启发了许多无监督方法[5]。无监督学习可以由以下的组合构建：  在没有任何活动时权重的自然增长与衰减； 

不依赖突触前脉冲到达，仅依靠突触后脉冲产生的作用 

短时程突触可塑性，突触前脉冲的作用 

突触前后脉冲的共同作用或突触前脉冲与突触后去极化的作用（一些研究发现当突触后脉冲不发放时，突触前脉冲与突触后膜电势也可以相互作用影响权重） 

上述所有作用都依赖于突触权重的当前值，即当权重接近最大值时其改变将会减小。



脉冲时序依赖可塑性（Spike Timing Dependent Plasticity, STDP）是Hebb无监督学习算法的一种变式。这一规则根据突触前后脉冲的相对时序来描述突触权重的变化。根据STDP规则，STP STD



我们从连续输出移动至二进制输出，这些脉冲训练的可解释性不强。但是，脉冲训练增强了我们处理时空数据（或者说真实世界感官数据）的能力。空间指神经元仅与附近的神经元连接，这样它们可以分别处理输入块（类似于 CNN 使用滤波器）。时间指脉冲训练随着时间而发生，这样我们在二进制编码中丢失的信息可以在脉冲的时间信息中重新获取。这允许我们自然地处理时间数据，无需 RNN 添加额外的复杂度。事实证明脉冲神经元是比传统人工神经元更强大的计算单元。



ANN   架构：连续的输入值通过突触加权传播进入神经元激活函数激活输出连续的值，学习规则：前向传播和BP以及GD不断更新参数

SNN 架构：输入数据或者外界刺激通过一定的编码方式编码成脉冲序列（离散的脉冲时间点组成的时间序列），通过脉冲神经元进行计算，输出脉冲并且进行解码输出。  学习规则：由于脉冲神经元的激活不是一个可微函数，所以不能进行梯度下降方法BP

更具有生物可解释性的脉冲神经网络，采用精确定时的脉冲序列来编码神经信息。神经网络内部的信息传递是由脉冲序列完成的，脉冲序列是由离散的脉冲时间点组成的时间序列，因此，在进行脉冲神经网络的模拟与计算时，包含以下步骤：①当输入数据或神经元受到外界刺激时，经过特定的脉冲序列编码方法，可将数据或外界刺激编码成特定的脉冲序列；②脉冲序列在神经元之间传递并经过一定的处理，处理之后将输出的脉冲序列通过特定的解码方法进行解码并给出具体的响应。







1. 每个神经元都是一个 **多输入单输出**的信息处理单元；

2. 神经元输入 **分兴奋性输入**和 **抑制性输入**两种类型；

3. 神经元具有 **空间整合特性**和 **阈值特性**；

4. 神经元输入与输出间有固定的 **时滞**，主要取决于突触延搁 

    

   神经元是大脑信息处理的最基本单元，神经元通过突触互相连接（结构），发送和接收动作电位进行交流，神经元输入 **分兴奋性输入**和 **抑制性输入**两种类型（学习）

   M-P模型就是科学家根据神经元结构和工作原理，构造的一种抽象简化的计算模型，也就是一个简单的神经元计算单元。

    *f*为激活函数，这儿使用阶跃函数，大于0时取1，小于等于0时取0。可以看出M-P模型就是一个加权求和再激活的过程，能够完成线性可分的分类问题。
   M-P模型的权值 W  和偏置 b  都是人为给定的，所以对此模型不存在"学习"的说法。这也是M-P模型与单层感知机最大的区别，感知机中引入了学习的概念，权值 W 和偏置 b 是通过学习得来。 

   之后在此基础上，发展了单层感知机， 在结构上单层感知机和M-P模型没有太大区别， M-P模型就是现在的一个神经元结构，但是没有参数学习的过程，单层感知机引入损失函数，并提出了学习的概念，多层感知机通过增加层数解决非线性问题，但是需要人为固定一层参数，只能训练其中一层。直到1986年Hinton提出了反向传播算法，使得训练多层网络成为可能。在GPU并行运算能力的大力发展下，网络的层数得以不断增加，新的网络模型也越来越多，感知机也逐渐退出了历史舞台，但了解一个领域的发展历史对理解这个领域还是很有用的。 



虽然传统神经网络已然在各项任务上取得了优异的成绩， 但它们的原理和运算过程仍然和真正的人脑信息处理过程依然相差甚远。主要的差异可以总结为以下几点：

**1**. 传统神经网络算法仍然依据于使用高精度的浮点数进行运算， 然而人脑并不会使用浮点数进行运算。 在人的传感系统和大脑中， 信息会以动作电压或称之为电脉冲（electric spike）的形式传递，接受，和处理。
**2**. ANN的训练过程对反向传播算法（梯度下降)的依赖程度非常之高， 然而在真实的人脑学习过程中，科学家们还没有观察到这种学习类型。 更多的， 人脑的记忆和学习依赖于突触后细胞受到刺激后所产生的突触可塑性。 详见： Hebbian learning
**3**. ANN通常需要大量的标签数据集来驱动网络的拟合。 这与我们平时经理的有所不同。 我们在很多情况下的感知和学习过程都是非监督式的。并且， 人脑通常不需要如此大量反复的数据来学习同一件事情。

综上所述， 为了使神经网络更加接近于人脑， SNN随而诞生了。发现它的灵感，就来自于生物大脑处理信息的方式—spikes。

 那么PSP是什么呢？ 简单的解释就是神经元上的膜电压变化。例如图三就是一个神经元接受到spike后膜电压 u ( t ) u(t)*u*(*t*)随着时间 t t*t* 的变化。在生物学中， 这种电脉冲的幅值和时间常数是在特定范围内的， 例如图三中， 膜电压在接受到脉冲输入前会一直保持在 − 70 -70−70mV 的地方， 这个值通常叫做静止值（resting value）。 当接受到刺激后， 会产生电压变化的幅值。 在变化结束后， 膜电压会归位回起始的静止值。在实际操作中这种波形要怎样产生呢？ 科学家们对生物神经元进行实验分析后， 给出了许多神经元的差分方程（differential equation）模型: 





众所周知，神经元是大脑处理信息的最基本单位，神经元与神经元之间由突触连接，通过发送和接收动作电位（脉冲）进行交流，输入一般分为兴奋性输入和抑制性输入，这跟后面涉及到的突触可塑性有很大的关系。神经元还有一个很大的特性，就是阈值，输入经过神经元的计算只有达到特定阈值才会产生一定的刺激

基于此，科学家们早期仿照神经元的结构和工作原理提出了M-P模型，构造了一个抽象和简化的计算模型单元。如图所示，就是一个简单的加权求和再通过计算的过程，欧米伽是它们连接突触上的权重，M-P模型的权重是人工设定，不可改变的。之后人们又将M-P模型改进，提出了单层感知机，还是如图所示，它和M-P模型的区别在于，它突触上的权重是可以通过引入一定的学习规则改变的。多层感知机就是在单层感知机的基础上堆叠起来的。这是第一代ANN。但他们只能用来拟合一些线性相关的计算方式。对于非线性的，它们也束手无策。于是人们在神经元的输出引入了非线性的激活函数，可以用来拟合一些非线性的表达式。实现更多的功能。之后的CNN,RNN DNN都是在此基础上进行发展的，并且随着GPU和大量标签化数据的出现。使得第二代人工神经网络在很多任务上有着不俗的表现。然后，第二代人工神经网络跟现实生物人脑处理信息的结构和计算方式上还是存在一定了差距。ANN还是依据高精度浮点数进行计算，然而人脑是没有进行浮点数运算的过程。人脑进行信息处理的单元是脉冲。ANN还依赖于BP和GD进行更新参数，而人脑的记忆和学习是通过神经元接收到刺激后产生的突触可塑性。